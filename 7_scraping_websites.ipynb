{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94654e7-79e9-4aaa-b963-363b7c1206fb",
   "metadata": {},
   "source": [
    "## Processing PDFs\n",
    "\n",
    "This lesson will go over how to scrape the UC Berkeley Police website for latest crime statistics, then extract the data from a PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca5392-8ca8-4a0a-9b0e-81ac426ed8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "import requests\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f818b88-b06c-4378-b831-602d9cef16f5",
   "metadata": {},
   "source": [
    "Whenever you access a website, your computer sends a bit of data about itself called headers to that website, including which web browser you're using, and where you're coming from. We will spoof the headers we're using to the actual organization we represent, since we're doing it from a server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4063841-4cef-4869-b5c2-867daa42cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'referer': 'https://journalism.berkeley.edu/',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c857e72-3c34-423b-97fa-9bd83f3e1503",
   "metadata": {},
   "source": [
    "If you visit [https://ucpd.berkeley.edu/alerts-data/daily-crime-log](https://ucpd.berkeley.edu/alerts-data/daily-crime-log) website, you'll see that all of the data is in a hard-to-access format of PDFs. This is common with many government agencies. Instead of manualy downloading all of these PDFs one-by-one, we can programmatically scrape the website and download all of the PDFs with code. \n",
    "\n",
    "We wil use the **requests** Python library, which is specifically built for requesting information from other web properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fdd962-801f-44a0-a786-56d813f25a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = requests.get(\"https://ucpd.berkeley.edu/alerts-data/daily-crime-log\", headers=headers, timeout=4)\n",
    "webpage.encoding = 'utf-8'\n",
    "webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ea161-8541-46a7-a7ea-285e4792eb97",
   "metadata": {},
   "source": [
    "Next, we will use a common Python web scraping parser called Beautiful Soup to make sense of the webpage text string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218a41e-b425-4ee6-924c-543c88d9f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599c718-60e2-4918-a643-d1729c42d2c8",
   "metadata": {},
   "source": [
    "Much cleaner. Now we just need to find all of the `<a>` tags that link to a PDF file. First, let's find all of the \"a\" tags that have an href attribute. BeautifulSoup will store them in a Python list for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec77c22-5f79-4fb6-8955-bcb5d286e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = soup.find_all('a', href=True)\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd29e5a-b4f9-4d64-a911-085a8695f7be",
   "metadata": {},
   "source": [
    "Great, now we just need to create a For Loop for going through each URL and create a new python list of only the URL portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624453-9ac0-4ee2-ae2d-3bcbda3b9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = []\n",
    "\n",
    "for url in urls:\n",
    "    if(url['href'].endswith(\".pdf\")):\n",
    "        pdfs.append(url['href'])\n",
    "\n",
    "pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3ae1b-f72a-4c94-be2e-c8e8a7c88807",
   "metadata": {},
   "source": [
    "That first item in the pdfs list is an errant document. We can use the `pop` method to get rid of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600d2c3-3e63-47a4-99d5-e285416b31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs.pop(0)\n",
    "pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2258e5f-b108-4473-9c00-07cd590f4af2",
   "metadata": {},
   "source": [
    "Before we download all of these PDFs, we need to be aware of what rate limits the server might have. Servers will block scrapers who request files too frequently in a row. Many websites list their policies on a special files called robots.txt that exist on the root server of their website. Let's review UCPDs policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fbd23-bd51-4ad3-9b5c-73cf8ff542b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.get(\"https://ucpd.berkeley.edu/robots.txt\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b062ef5-1bd6-4b1b-93b4-aaab7aa2edde",
   "metadata": {},
   "source": [
    "Now that we know it's 10 seconds, we will wait 10 seconds between each request. This for loop will download each file and save it to our server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901c807-0e3a-421e-a2fe-3665655674b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in pdfs:\n",
    "    try:\n",
    "        response = requests.get(pdf, headers=headers)\n",
    "        response.raise_for_status() \n",
    "        \n",
    "        filename = os.path.basename(pdf)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f'Downloaded: {filename}')\n",
    "        time.sleep(10)  # wait 10 seconds between each request\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {pdf}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541ae45-4fd1-465e-9310-899f1bf27238",
   "metadata": {},
   "source": [
    "Let's take the first file, and try to scrape it using a utility called PDF Plumber. Replace the pdf_name for one of your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a3f6c-effa-4681-9a5b-d3366fd3818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = \"dcl20250401.pdf\"\n",
    "\n",
    "pdf = pdfplumber.open(pdf_name)\n",
    "page = pdf.pages[0]\n",
    "image = page.to_image(resolution=150)\n",
    "image.reset().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a5fd8-cac7-4ac2-8a90-1765a1fcc26e",
   "metadata": {},
   "source": [
    "This shows us the auto detect feature of the table structure. Unfortunately, the lines on the table are partially broken and the system isn't able to find the table very well. But we can help it along by cropping the image to just the table portion, and explicitly stating where the vertical lines are in this table. PDF Plumber does the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f9f1c-d582-4c61-b7db-50e19cf147af",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_settings = {\"explicit_vertical_lines\":[10, 73, 387, 470, 695]}\n",
    "crop_settings  = (0, 140,792,580)\n",
    "\n",
    "image = page.crop(crop_settings).to_image(resolution=150)\n",
    "image.reset().debug_tablefinder(table_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44ec86-f991-463a-b0cf-1f7c33ae5f88",
   "metadata": {},
   "source": [
    "Much better. Now we can extract the text of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c02733-4114-4027-9e40-8a97845fda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_text = page.crop(crop_settings).extract_table(table_settings)\n",
    "table_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ef910-5456-4678-b50c-94358a1e2042",
   "metadata": {},
   "source": [
    "Our data is in a 2-dimensional Python list, which is perfect for importing to a Pandas DataFrame. We just need to list the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0677a8-aad0-44ca-8cb9-6d6ca1ed0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Case\", \"Crimes\", \"Reported\", \"Occurred Range\", \"Location\"]\n",
    "\n",
    "df = pd.DataFrame(table_text, columns=columns)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f437bf-f391-4688-9605-c874b607fb52",
   "metadata": {},
   "source": [
    "Lastly, we can modify out code to create a loop to go through every page of the pdf, and even every pdf file, combining all of the data together into a single spreadsheet. This is beyond the scope of this lesson, so we'll just save this page we extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac8084-610a-4749-a5bb-9da8cfd91762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"first_page_exported.csv\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
